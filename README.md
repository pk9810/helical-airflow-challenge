# Helical Airflow Challenge -- Workflow Orchestration + Observability

This project implements a containerised machine-learning workflow using:

-   Apache Airflow for orchestration
-   Docker for isolated model execution
-   Prometheus + Grafana for observability
-   A custom Helical Runner container that executes `run_helical.py`
-   Shared data volume for reading `.h5ad` datasets

Everything runs fully locally using docker-compose.

## üöÄ What This System Does

This workflow automates end-to-end execution of a Helical ML pipeline
using Airflow + Docker + Prometheus + Grafana.

### 1Ô∏è‚É£ User triggers a DAG in Airflow UI

Go to: <http://localhost:8080>

**Login credentials**

    Username: admin
    Password: admin

Then:

-   Open `helical_model_workflow`
-   Click ‚ñ∂ Trigger DAG

### 2Ô∏è‚É£ Airflow schedules the task

Airflow internal flow:

    Webserver ‚Üí Scheduler ‚Üí Worker

Worker executes the DockerOperator task.

### 3Ô∏è‚É£ DockerOperator starts the Helical Runner container

The operator launches the container:

-   Image: `helical-runner:latest`
-   Mounts dataset folder:


    ./data ‚Üí /data (inside container)

Runs:

    python run_helical.py

### 4Ô∏è‚É£ run_helical.py performs the ML pipeline

Inside the container:

-   Loads `.h5ad` dataset(s) from `/data`
-   If empty ‚Üí loads Scanpy PBMC3K dataset
-   Shrinks dataset for fast CPU execution
-   Runs scGPT embeddings (Helical SDK)
-   Optionally trains a tiny classifier
-   Writes metrics ‚Üí `/tmp/metrics`
-   Logs visible inside Airflow UI

### 5Ô∏è‚É£ Observability Metrics Flow

    Airflow
       ‚Üì
    StatsD
       ‚Üì
    StatsD Exporter
       ‚Üì
    Prometheus
       ‚Üì
    Grafana

Prometheus scrapes:

-   Airflow task duration & failures\
-   cAdvisor container CPU/Mem\
-   StatsD metrics\
-   Prometheus internal metrics

Grafana datasource:

    URL: http://prometheus:9090

------------------------------------------------------------------------

# ‚ñ∂ How To Run Everything (Copy-Paste Friendly)

Follow these steps exactly in order.

------------------------------------------------------------------------

## ‚úÖ 1Ô∏è‚É£ (Optional) Install local Conda environment

    chmod +x setup_helical_env.sh
    ./setup_helical_env.sh

## ‚úÖ 2Ô∏è‚É£ Initialise Airflow database (first time only)

üî• IMPORTANT --- MUST RUN BEFORE ANYTHING ELSE

    docker compose up airflow-init

This sets up:

-   Airflow metadata DB\
-   Admin user\
-   Initial migrations

Stop it (Ctrl + C) after completion.

------------------------------------------------------------------------

## ‚úÖ 3Ô∏è‚É£ Start full system (Airflow + Prometheus + Grafana)

    docker compose up -d

### Services Available

  Service           URL
  ----------------- -------------------------------
  Airflow Web UI    http://localhost:8080
  
  Prometheus        http://localhost:9090
  
  Grafana           http://localhost:3000
  
  StatsD Exporter   http://localhost:9102/metrics
  
  cAdvisor          http://localhost:8081
  

------------------------------------------------------------------------

## ‚úÖ 4Ô∏è‚É£ Build the Helical Runner container

    docker build -t helical-runner:latest ./helical-container

This image contains:

-   Helical SDK\
-   scGPT deps\
-   Scanpy + AnnData\
-   `run_helical.py`

------------------------------------------------------------------------

## ‚úÖ 5Ô∏è‚É£ Add datasets (optional)

Place `.h5ad` files in:

    ./data/

These map to `/data` inside the container.

If none provided ‚Üí auto-loads PBMC3K dataset.

------------------------------------------------------------------------

## ‚úÖ 6Ô∏è‚É£ Trigger the Helical DAG in Airflow

1.  Visit: http://localhost:8080\
2.  Unpause: `helical_model_workflow`\
3.  Click "Trigger DAG"\
4.  View logs:\
    `run_helical_model ‚Üí View Log`

------------------------------------------------------------------------

# üìä Observability Setup

## Prometheus

Scrapes:

-   StatsD Exporter (`airflow_*`)
-   cAdvisor (container CPU/Mem)
-   Prometheus internal metrics

Config file:

    monitoring/prometheus.yml

## Grafana

Add datasource:

    URL: http://prometheus:9090

Dashboards can show:

-   DAG run durations\
-   Task success/failure counts\
-   Container CPU/memory usage\
-   Airflow scheduler uptime\
-   Worker activity & queue depth
