ğŸš€ Helical Airflow Challenge
Containerized Workflow Orchestration with Airflow, Docker, Prometheus, Grafana & cAdvisor

This project implements a containerized workflow orchestration system designed for the Helical Technical Challenge.
It features:

Apache Airflow running with the Celery Executor

Dockerized Helical model execution inside an Airflow DAG

Structured data mounting (/opt/data)

Prometheus-based observability pipeline

StatsD â†’ statsd-exporter â†’ Prometheus â†’ Grafana

Container-level resource monitoring via cAdvisor

Fully repeatable local environment setup script (Conda + Helical installation)

ğŸ“ Project Structure
helical-airflow-challenge/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ helical_model_workflow.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ data/                       # Mounted into the model container
â”‚   â””â”€â”€ sample.h5ad
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚
â”œâ”€â”€ setup_helical_env.sh        # One-click local setup & Helical installation
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

âš™ï¸ 1. Environment Setup (One-Click Script)

Run this script on any machine for the first-time setup:

chmod +x setup_helical_env.sh
./setup_helical_env.sh

The script automatically:

âœ” Detects OS
âœ” Installs Miniconda (if missing)
âœ” Creates Conda env helical-package
âœ” Installs Helical (PyPI + GitHub latest)
âœ” Ensures Python 3.11.13
âœ” Installs optional extensions
âœ” Automatically activates the environment in new terminals

ğŸ³ 2. Start Full Docker Orchestration

Ensure Docker is installed.

Start everything:
docker compose up -d --build

Stop everything (including volumes):
docker compose down -v

ğŸ“¦ 3. Airflow Architecture

This setup includes:

Component	Purpose
Airflow Webserver	UI & DAG management
Airflow Scheduler	Orchestrates DAG tasks
Airflow Worker (Celery)	Executes tasks
Postgres	Airflow metadata DB
Redis	Celery broker
Docker provider	Allows Airflow to execute Helical container

All tasks share a mounted folder:

host: ./data  â†’  container: /opt/data

ğŸ§¬ 4. The Helical Model DAG

A sample DAG is included at:

airflow/dags/helical_model_workflow.py


It performs:

start â€“ empty task

run_helical_model â€“ runs a Docker container

end â€“ empty task

The Docker task mounts:

/opt/data â†’ /opt/data  (inside container)


You can swap this with any Helical model:

image="helicalai/helical:latest"
command="python3 examples/run_model.py --input /opt/data/sample.h5ad"

ğŸ“Š 5. Observability Pipeline

This project includes full metrics stack:

Airflow â†’ StatsD â†’ statsd-exporter â†’ Prometheus â†’ Grafana â†’ Dashboards
                           â†‘
                cAdvisor â†’ Prometheus

Prometheus Targets
Target	Purpose
statsd-exporter:9102	Airflow metrics
cadvisor:8080	Container CPU / Memory metrics
prometheus:9090	Self-metrics
Access URLs
Service	URL
Airflow UI	http://localhost:8080

Prometheus	http://localhost:9090

Grafana	http://localhost:3000

cAdvisor	http://localhost:8081
ğŸ“ˆ 6. Grafana Dashboards

Grafana automatically loads "Heical â€“ Airflow & Containers" dashboard.

Panels available:

Airflow Scheduler Heartbeat

DAG Runs Count / Success / Duration

Task Duration (p95)

Worker CPU / Memory (via cAdvisor)

Per-container resource usage

Login credentials:

Username: admin
Password: admin

ğŸ“‘ 7. Metrics Configuration
StatsD in Airflow
AIRFLOW__METRICS__STATSD_ENABLED: "True"
AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
AIRFLOW__METRICS__STATSD_PORT: "9125"
AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
AIRFLOW__METRICS__STATSD_ALLOW_LIST: "*"

Prometheus scrapes statsd-exporter:
- job_name: "airflow"
  static_configs:
    - targets: ["statsd-exporter:9102"]

ğŸ³ 8. Docker Compose Overview

Key services included:

Service	Description
airflow-webserver	Main UI
airflow-scheduler	DAG scheduling
airflow-worker	Task workers
statsd-exporter	Metric bridge
prometheus	Metrics storage
grafana	Visualization
cadvisor	Container monitoring
redis	Celery broker
postgres	Metadata DB
ğŸš¦ 9. Triggering a DAG

Visit:

â¡ http://localhost:8080

Enable DAG â†’ Click "Play" â†’ Trigger DAG.

Prometheus & Grafana will show metrics once the DAG runs.

ğŸ§ª 10. Verification Commands
Check exporter metrics:
curl http://localhost:9102/metrics | grep airflow

Check cAdvisor:
curl http://localhost:8081/metrics | head

Check Prometheus UI:
open http://localhost:9090/targets

Check Prometheus queries:

statsd_airflow_scheduler_heartbeat

statsd_airflow_dagrun_success_total

container_cpu_usage_seconds_total

ğŸ› ï¸ 11. Common Issues & Fixes
âŒ No Airflow metrics in Prometheus

âœ” Ensure Airflow sends StatsD â†’ port 9125, not 8125.

âŒ Grafana shows no data

âœ” Trigger a DAG so metrics begin flowing.

âŒ Docker provider missing

âœ” Ensure _PIP_ADDITIONAL_REQUIREMENTS includes:

apache-airflow-providers-docker
statsd

âŒ Permission denied reading data files

âœ” Ensure ./data is readable by Docker.

ğŸ“Œ 12. Future Improvements

Add MLflow tracking for model metadata

Run Airflow with LocalKubernetesExecutor

Add Loki + Promtail for central log aggregation

Add model validation tasks in DAG